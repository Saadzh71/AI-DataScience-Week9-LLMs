{"cells":[{"cell_type":"markdown","metadata":{"id":"m83H0SRDK01q"},"source":["# Traffic Violation RAG System\n","In this exam, you will implement a Retrieval-Augmented Generation (RAG) system that uses a language model and a vector database to answer questions about traffic violations. The goal is to generate answers with relevant data based on a dataset of traffic violations and fines.\n","\n","Here are helpful resources:\n","* [LangChain](https://www.langchain.com/)\n","* [groq cloud documentation](https://console.groq.com/docs/models)\n","* [LangChain HuggingFace](https://python.langchain.com/docs/integrations/text_embedding/sentence_transformers/)\n","* [Chroma Vector Store](https://python.langchain.com/docs/integrations/vectorstores/chroma/)\n","* [Chroma Website](https://docs.trychroma.com/getting-started)\n","* [ChatGroq LangChain](https://python.langchain.com/docs/integrations/chat/groq/)\n","* [LLM Chain](https://api.python.langchain.com/en/latest/chains/langchain.chains.llm.LLMChain.html#langchain.chains.llm.LLMChain)\n","\n","Dataset [source](https://www.moi.gov.sa/wps/portal/Home/sectors/publicsecurity/traffic/contents/!ut/p/z0/04_Sj9CPykssy0xPLMnMz0vMAfIjo8ziDTxNTDwMTYy83V0CTQ0cA71d_T1djI0MXA30gxOL9L30o_ArApqSmVVYGOWoH5Wcn1eSWlGiH1FSlJiWlpmsagBlKCQWqRrkJmbmqRqUZebngB2gUJAKdERJZmqxfkG2ezgAhzhSyw!!/)\n","\n","Some installs if needed:\n","```python\n","!pip install langchain-huggingface langchain langchain-community langchain-chroma Chroma langchain-groq\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U5MtVtxvxqd7"},"outputs":[],"source":["!kaggle datasets download -d khaledzsa/dataset\n","!unzip dataset.zip"]},{"cell_type":"markdown","metadata":{"id":"IKe3G7bqK-W6"},"source":["## Step 1: Install Required Libraries"]},{"cell_type":"markdown","metadata":{"id":"ewOnaf7BLBQ8"},"source":["To begin, install the necessary libraries for this project. The libraries include `LangChain` for building language model chains, and `Chroma` for managing a vector database."]},{"cell_type":"code","source":[],"metadata":{"id":"sjiPx3oXTfWT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QI7_KEjILJZ8"},"source":["## Step 2: Load the Traffic Violations Dataset"]},{"cell_type":"markdown","metadata":{"id":"FY6U8FxlLLON"},"source":["You are provided with a dataset of traffic violations. Load the CSV file into a pandas DataFrame and preview the first few rows of the dataset using `.head()`. You can also try and see the dataset's characteristics."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PzTMfTyJ_tZG"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"0Hs28tz2LbFx"},"source":["## Step 3: Create Markdown Content from the Dataset"]},{"cell_type":"markdown","metadata":{"id":"PiNAqLhELd_S"},"source":["For each traffic violation in the dataset, you will generate markdown text that describes the violation and the associated fine. Create a loop to iterate through the dataset and store the generated markdown in a list. Each fine should look like this:\n","\n","**المخالفة** - الغرامة"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"daP6wjbA_km0"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"ifkMDS5SLui4"},"source":["## Step 4: Chunk the Markdown Data"]},{"cell_type":"markdown","metadata":{"id":"pJxNEV5yLxMu"},"source":["Using LangChain's `RecursiveCharacterTextSplitter`, split the markdown texts into smaller chunks that will be stored in the vector database."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hf3-3j9iALUN"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"EviXuMjfL2Gj"},"source":["## Step 5: Generate Embeddings for the Documents"]},{"cell_type":"markdown","metadata":{"id":"gAUq12UtL5OJ"},"source":["Generate embeddings for the chunks of text using HuggingFace's pre-trained Arabic language model. These embeddings will be stored in a `Chroma` vector store."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d4-YlqMKAeGr"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"l32elHl2L-ob"},"source":["## Step 6: Define the RAG Prompt Template"]},{"cell_type":"markdown","metadata":{"id":"X1zWp3KfMAld"},"source":["Define a custom prompt template in Arabic to retrieve traffic violation-related answers based on the context. Ensure the template greets the user first, states that the information provided could be incorrect, and advises the user to visit the traffic initiative website to verify. Additionally, provide the user with advice in Arabic, ensuring it stays within the given context."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gdy4qbn_CYTn"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"IvfcCIjgMG34"},"source":["## Step 7: Initialize the Language Model"]},{"cell_type":"markdown","metadata":{"id":"_lvHUsNTMIvX"},"source":["Initialize the language model using the Groq API. Set up the model with a specific configuration, including the API key, temperature setting, and model name."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EM_tszFvHN6m"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"c3T-2Fy9MLPa"},"source":["## Step 8: Create the LLM Chain"]},{"cell_type":"markdown","metadata":{"id":"fCcrmiA2MOOi"},"source":["Now, you will create an LLM Chain that combines the language model and the prompt template you defined. This chain will be used to generate responses based on the retrieved context."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j1EEjdquHrTL"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"di48NYGQMQtS"},"source":["## Step 9: Implement the Query Function"]},{"cell_type":"markdown","metadata":{"id":"huXN44hwMS07"},"source":["Create a function `query_rag` that will take a user query as input, retrieve relevant context from the vector store, and use the language model to generate a response based on that context."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HJLrEKqzHhDy"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"9iRfIjyzPLC_"},"source":["## Step 10: Inference - Running Queries in the RAG System"]},{"cell_type":"markdown","metadata":{"id":"0iTaUjpWPOyt"},"source":["In this final step, you will implement an inference pipeline to handle real-time queries. You will allow the system to retrieve the most relevant violations and fines based on a user's input and generate a response.\n","\n","1. Inference Workflow:\n","\n","  * The user inputs a query (e.g., \"ماهي عقوبة عدم الوقوف وقوفاً تاماً عند إشارة؟\").\n","  * The system searches for the most relevant context from the traffic violation vector store.\n","  * It generates an answer and advice based on the context.\n","\n","2. Goal:\n","  * Run the inference to answer questions based on the traffic violation dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7k4BmIAHH38X"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"collapsed_sections":["IKe3G7bqK-W6"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}