{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-VUJ3aeArCl"
      },
      "source": [
        "# Tutorial on LangChain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mw5HbmQQArCm"
      },
      "source": [
        "LangChain is a powerful framework designed to assist developers in creating applications that integrate with large language models (LLMs) like OpenAI's GPT-4, LLaMA 3.1, and mmany other LLMs. By using LangChain, you can build systems capable of handling complex tasks, maintaining memory across conversations, and chaining responses, allowing for richer interactions.\n",
        "\n",
        "In this tutorial, we will cover how to:\n",
        "\n",
        "* Install LangChain and dependencies.\n",
        "* Understand and use the basic concepts of LangChain.\n",
        "* Build a simple LangChain application that interacts with GPT.\n",
        "* Create conversational agents.\n",
        "* Use memory for conversation history.\n",
        "* Integrate external data sources, such as APIs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-fXMcoQArCn"
      },
      "source": [
        "# 1. Installation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xXGcHEoArCn"
      },
      "source": [
        "Before we begin using LangChain, you need to install it alongside the required dependencies, like langchain-groq for accessing GROQ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qItGp-9EArCn",
        "outputId": "63161566-4bb4-4c84-a282-e13a34b19714"
      },
      "outputs": [],
      "source": [
        "!pip install langchain langchain-community langchain-groq duckduckgo-search geopy requests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1766xlJXArCo"
      },
      "source": [
        "# 2. Key Concepts in LangChain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qe5bjd1JArCo"
      },
      "source": [
        "LangChain revolves around several core concepts that allow you to build flexible and powerful applications. Here’s a quick overview of each:\n",
        "\n",
        "## A. LLMs (Large Language Models)\n",
        "LangChain is built to work with large language models like GPT-4. These models are responsible for generating responses based on the input they receive. By using LangChain, you can control how to query and process responses from these models.\n",
        "\n",
        "## B. Chains\n",
        "Chains are the primary building blocks in LangChain. A chain could be a simple sequence of operations, like querying the LLM, or something more complex, involving multiple LLM calls, conditionals, and external APIs.\n",
        "\n",
        "## C. Agents\n",
        "Agents are decision-making systems that interact with users and decide the best action to take. An agent can process user queries and choose between calling different APIs, querying the LLM, or performing computations.\n",
        "\n",
        "## D. Memory\n",
        "LangChain allows you to store conversation history using the memory module. This feature is particularly useful in building conversational systems where past interactions are relevant to future responses."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mbjq77GArCp"
      },
      "source": [
        "# 3. Building a Basic LangChain Application"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rU3wSZPnArCp"
      },
      "source": [
        "## Step 1: Set Up the LLM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iObZl0IfArCp"
      },
      "source": [
        "First, you need to configure GROQ API so that LangChain can communicate with the LLaMA model. You can either set the API key as an environment variable or pass it directly into your code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YhBLSZSwArCq"
      },
      "outputs": [],
      "source": [
        "groq_api_key = \"your_api_key\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To obtain an API key from Groq, follow these steps:\n",
        "\n",
        "1. Visit the official [Groq website](https://groq.com/).\n",
        "2. In the top navigation bar, go to the `Developers` section and select [`Start Building`](https://console.groq.com/login). This will direct you to log in to Groq's console.\n",
        "3. Once logged in, click on the [`API Keys`](https://console.groq.com/keys) tab in the left-hand menu.\n",
        "4. Click the `Create API Key` button and provide a name for your new key.\n",
        "5. Copy the generated API key immediately, as it will not be available for copying later. You're all set!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "llfE9FLcArCq"
      },
      "outputs": [],
      "source": [
        "from langchain_groq import ChatGroq\n",
        "\n",
        "llm = ChatGroq(\n",
        "    model=\"llama-3.1-8b-instant\",\n",
        "    temperature=0.8,\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=2,\n",
        "    api_key=groq_api_key\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeK4jBwpArCq"
      },
      "source": [
        "Here, we’re setting the `temperature` to 0.7. The temperature controls the randomness of the output, with lower values generating more focused results and higher values producing more diverse responses."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FANFH9ArArCq"
      },
      "source": [
        "## Step 2: Create a Prompt Template"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wngb7MN0ArCq"
      },
      "source": [
        "Prompt templates in LangChain allow you to build dynamic prompts that are passed to the LLM. Let’s create a template that will generate a story based on user input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "QGcv9G0KArCq"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# Define the prompt template\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"topic\"],\n",
        "    template=\"Write a short story about {topic}.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ch1O--3bArCq"
      },
      "source": [
        "This prompt takes one input variable `topic` and generates a story about the topic entered by the user."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OjZjpRwArCq"
      },
      "source": [
        "## Step 3: Create an LLM Chain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EixqXF_KArCq"
      },
      "source": [
        "Next, we’ll create an LLM Chain that combines the LLM with our prompt template. This chain will execute the prompt and retrieve the model’s response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2sfLbyYArCq",
        "outputId": "11a507c6-ad8c-4d56-bcf4-25a65539b46b"
      },
      "outputs": [],
      "source": [
        "from langchain import LLMChain\n",
        "\n",
        "# Create an LLM chain\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "# Run the chain with a specific topic\n",
        "response = chain.run(\"a brave knight\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TdY9PzZArCr"
      },
      "source": [
        "When you run the code, it will generate a short story about \"a brave knight.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nowF_WX8ArCr"
      },
      "source": [
        "# 4. Building a Conversational Agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K97FtjpHArCr"
      },
      "source": [
        "## Use LangChain pre-built tools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gL7AeODCArCr"
      },
      "source": [
        "Agents in LangChain allow your application to make decisions on which actions to take based on the user’s query. For example, an agent might decide to call an external API, retrieve data from a database, or query the LLM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xfa50ryxArCr"
      },
      "source": [
        "### Step 1: Define External Tools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfqUbvuBArCr"
      },
      "source": [
        "Lets select a tool from [LangChain's documentation](https://python.langchain.com/v0.2/docs/integrations/tools/), for this example lets use the [DuckDuckGo](https://python.langchain.com/v0.2/docs/integrations/tools/ddg/) tool as a search function to give the LLM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTIzH7AGArCr"
      },
      "source": [
        "First, lets explore the tool!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "0ploKpIyArCr"
      },
      "outputs": [],
      "source": [
        "from langchain_community.tools import DuckDuckGoSearchRun\n",
        "\n",
        "search = DuckDuckGoSearchRun()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXRQdjuIArCr"
      },
      "source": [
        "This is the default name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Ju4pTx22ArCr",
        "outputId": "ab0a167a-4782-4a4e-c4f8-f1e4e72b4fd1"
      },
      "outputs": [],
      "source": [
        "search.name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHU0tksvArCr"
      },
      "source": [
        "This is the default description"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "emeqytOvArCr",
        "outputId": "33203249-d04a-4326-df37-88ab96d16d52"
      },
      "outputs": [],
      "source": [
        "search.description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7IjIVJEArCr"
      },
      "source": [
        "This is the default JSON schema of the inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nclkDfA0ArCs",
        "outputId": "724fe7f5-057d-4de8-b168-f1f1e12415e9"
      },
      "outputs": [],
      "source": [
        "search.args"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8N-R2nUArCs"
      },
      "source": [
        "We can see if the tool should return directly to the user"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MetUo9CArCs",
        "outputId": "b427230e-c342-4d4f-db02-6edd87c3fc15"
      },
      "outputs": [],
      "source": [
        "search.return_direct"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trRXAt89ArCs"
      },
      "source": [
        "We can call this tool with a dictionary input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "id": "bg3wr79JArCs",
        "outputId": "d6145a87-0ffe-409f-9023-c3d5634ff87f"
      },
      "outputs": [],
      "source": [
        "search.invoke({\"query\": \"What is Tuwaiq Academy?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHgj6QzxArCs"
      },
      "source": [
        "Or simply by passing the query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "id": "QCkJk2iUArCu",
        "outputId": "67a9b624-cea4-4ba3-ccec-93105712cb25"
      },
      "outputs": [],
      "source": [
        "search.invoke(\"What is Tuwaiq Academy?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEn6mMTvArCv"
      },
      "source": [
        "### Step 2: Set Up the Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_tm4spK4ArCv"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import initialize_agent, AgentType\n",
        "\n",
        "# List of tools the agent can use\n",
        "tools = [search]\n",
        "\n",
        "# Initialize the agent\n",
        "agent = initialize_agent(\n",
        "    tools=tools,\n",
        "    llm=llm,\n",
        "    agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WiFC1vEVArCv"
      },
      "source": [
        "The agent uses **ZERO_SHOT_REACT_DESCRIPTION**, which allows it to decide which tool to use based on the input."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gILAdMb8ArCv"
      },
      "source": [
        "### Step 3: Run the Agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrLm8mp0ArCv"
      },
      "source": [
        "Now that the agent is set up, let’s run it and see how it responds to a query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "goumBXmzArCv",
        "outputId": "54f316f5-9db2-4f17-e098-348fd22eb6d0"
      },
      "outputs": [],
      "source": [
        "response = agent.run(\"Find information about SDAIA T5.\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0Sv1OpNArCv"
      },
      "source": [
        "The agent will decide to call the search tool and return the search results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nT8hOPk-ArCv"
      },
      "source": [
        "## Create & Use Custom Tools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWbU5L6ZArCv"
      },
      "source": [
        "### Step 1: Define External Tools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAT1x_A_ArCv"
      },
      "source": [
        "Let’s first define a tool that the agent can use. For demonstration purposes, we’ll create a simple function that gets the weather in a City."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPK324EuArCv"
      },
      "source": [
        "For that we will use [open meteo API](https://open-meteo.com/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvSmPLgjArCv",
        "outputId": "0447b261-e84e-4c15-c75e-03464ebdb45a"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "# We need to define the latitude and longitude for the location, this will be gevin to meteo API to ge the weather\n",
        "latitude = 35.6895  # Tokyo, Japan latitude\n",
        "longitude = 139.6917  # Tokyo, Japan longitude\n",
        "\n",
        "# Open-Meteo API endpoint, we need to specify the latitude and longitude for the desired city in order to use this API\n",
        "url = f\"https://api.open-meteo.com/v1/forecast?latitude={latitude}&longitude={longitude}&current_weather=true\"\n",
        "\n",
        "# Send a GET request to the Open-Meteo API, the response will contain a JSON response containing the needed information or error if one occured\n",
        "response = requests.get(url)\n",
        "\n",
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0EfidRyArCv"
      },
      "source": [
        "the response was 200 which translated to OK, we will need to get the JSON response. Lets take a look on what the response look like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BiMyboQCArCw",
        "outputId": "d41079e3-b4ba-42eb-962d-bf354b23414e"
      },
      "outputs": [],
      "source": [
        "weather_data = response.json()\n",
        "weather_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JR2UT-U1ArCw"
      },
      "source": [
        "Thats a lot of information, we will pick a few of them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-Zrb8XhArCw",
        "outputId": "94fa7b1f-f336-48fa-9b41-c27ec90d61ca"
      },
      "outputs": [],
      "source": [
        "# Check if the request was successful, 200 will be translated to OK\n",
        "if response.status_code == 200:\n",
        "    # Parse the JSON data from the response\n",
        "    weather_data = response.json()\n",
        "\n",
        "    # Extract relevant information from the response\n",
        "    current_weather = weather_data.get('current_weather', {})\n",
        "    temperature = current_weather.get('temperature')\n",
        "    windspeed = current_weather.get('windspeed')\n",
        "    winddirection = current_weather.get('winddirection')\n",
        "    weather_time = current_weather.get('time')\n",
        "\n",
        "    # Print the weather information\n",
        "    print(f\"Current weather in Tokyo:\")\n",
        "    print(f\"Temperature: {temperature}°C\")\n",
        "    print(f\"Wind Speed: {windspeed} m/s\")\n",
        "    print(f\"Wind Direction: {winddirection}°\")\n",
        "    print(f\"Time of data: {weather_time}\")\n",
        "else:\n",
        "    print(\"Failed to retrieve weather data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wGqKSWmArCw"
      },
      "source": [
        "But we can't make the LLM decide what is the latitude and longitude of the city the user queried about, lets use `geopy` to get these information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUOL-n3IArCw"
      },
      "source": [
        "First lets create geolocator object and initialize `user_agent`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "zHaLhKxCArCw"
      },
      "outputs": [],
      "source": [
        "from geopy.geocoders import Nominatim\n",
        "geolocator = Nominatim(user_agent=\"LLMexercise\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOuKyJ_3ArCw"
      },
      "source": [
        "Now, lets get the latitude and longitude."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SArI72yPArCw",
        "outputId": "4e42da2a-f571-459e-ed05-93fbd286382b"
      },
      "outputs": [],
      "source": [
        "city_name = 'Riyadh Saudi Arabia '\n",
        "location = geolocator.geocode(city_name)\n",
        "location"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ig1RTLUBArCx"
      },
      "source": [
        "That is great, now lets extract the latitude and longitude."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-SvpKuTArCx",
        "outputId": "fdbfb22b-8910-443e-e3a3-62973012890e"
      },
      "outputs": [],
      "source": [
        "latitude = location.latitude\n",
        "longitude = location.longitude\n",
        "\n",
        "print(f'Latitude: {latitude}, longitude: {longitude}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOCYSRi8ArCx"
      },
      "source": [
        "Now lets try using these information with `meteo`, but lets also make a function that does all that and test it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kyq0LMtIArCx",
        "outputId": "62c4fa4f-1ed6-4ae6-bbbe-c7e053a52941"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from geopy.geocoders import Nominatim\n",
        "\n",
        "def get_weather_by_city(city_name: str):\n",
        "    # Initialize geocoder\n",
        "    geolocator = Nominatim(user_agent=\"LLMexercise\")\n",
        "\n",
        "    # Get location data (latitude and longitude) for the city\n",
        "    location = geolocator.geocode(city_name)\n",
        "\n",
        "    if location:\n",
        "        latitude = location.latitude\n",
        "        longitude = location.longitude\n",
        "    else:\n",
        "        return f\"City '{city_name}' not found.\"\n",
        "\n",
        "    # Open-Meteo API endpoint with the obtained latitude and longitude\n",
        "    url = f\"https://api.open-meteo.com/v1/forecast?latitude={latitude}&longitude={longitude}&current_weather=true\"\n",
        "\n",
        "    # Send a GET request to the Open-Meteo API\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # Check if the request was successful\n",
        "    if response.status_code == 200:\n",
        "        # Parse the JSON data from the response\n",
        "        weather_data = response.json()\n",
        "\n",
        "        # Extract relevant information from the response\n",
        "        current_weather = weather_data.get('current_weather', {})\n",
        "        temperature = current_weather.get('temperature')\n",
        "        windspeed = current_weather.get('windspeed')\n",
        "        winddirection = current_weather.get('winddirection')\n",
        "        weather_time = current_weather.get('time')\n",
        "\n",
        "        # Return the weather information as a formatted string\n",
        "        return (\n",
        "            f\"Current weather in {city_name}:\\n\"\n",
        "            f\"Temperature: {temperature}°C\\n\"\n",
        "            f\"Wind Speed: {windspeed} m/s\\n\"\n",
        "            f\"Wind Direction: {winddirection}°\\n\"\n",
        "            f\"Time of data: {weather_time}\"\n",
        "        )\n",
        "    else:\n",
        "        return \"Failed to retrieve weather data.\"\n",
        "\n",
        "city_name = \"Riyadh Saudi Arabia\"\n",
        "weather_info = get_weather_by_city(city_name)\n",
        "print(weather_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bV0u5zMArCx"
      },
      "source": [
        "That is fantastic! now lets give this function to the LLM to use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "3U6jLI7KArCx"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import Tool\n",
        "\n",
        "weather = Tool(\n",
        "    name=\"weather\",\n",
        "    func=get_weather_by_city,\n",
        "    description=\"Gets the weather of a city.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "zrfv-v2MArCx",
        "outputId": "0d95ccea-9973-474d-fea7-a823f88ddecf"
      },
      "outputs": [],
      "source": [
        "weather.name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "1ZpaLgc-ArCx",
        "outputId": "d867db4c-3a76-4241-b393-991692105db5"
      },
      "outputs": [],
      "source": [
        "weather.description"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8R8w8J3ArCx",
        "outputId": "e3a4feb6-a8f8-45d8-c76b-58ebb9aea91d"
      },
      "outputs": [],
      "source": [
        "weather.args"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvlTOIcNArCy"
      },
      "source": [
        "### Step 2: Set Up the Agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9hO5oJlArCy"
      },
      "source": [
        "We can now set up an agent that uses the weather tool. The agent will decide whether to query the LLM or use the weather tool based on the user’s input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "pOC4WqSAArCy"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import initialize_agent, AgentType\n",
        "\n",
        "# List of tools the agent can use\n",
        "tools = [weather]\n",
        "\n",
        "# Initialize the agent\n",
        "agent = initialize_agent(\n",
        "    tools=tools,\n",
        "    llm=llm,\n",
        "    agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PL-aBf9JArCy"
      },
      "source": [
        "The agent uses ZERO_SHOT_REACT_DESCRIPTION, which allows it to decide which tool to use based on the input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLKQFYfpArCy",
        "outputId": "cb0a54eb-98ff-437f-c4b0-77257ad3f1aa"
      },
      "outputs": [],
      "source": [
        "response = agent.run(\"What is the weather in Riyadh Saudi Arabia?\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37bROmSsArCz"
      },
      "source": [
        "Now we have LLM that is capable of checking the weather in Riyadh Saudi Arabia in real-time!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_r2BpLFkArCz"
      },
      "source": [
        "# 5. Using Memory for Conversations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nIV0SsHArCz"
      },
      "source": [
        "Memory is a key feature when building conversational systems that need to remember past interactions. LangChain allows you to use various types of memory to retain conversation history."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ADPqDYsArCz"
      },
      "source": [
        "## Using ConversationBufferMemory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRrhdtdpArCz"
      },
      "source": [
        "### Step 1: Set Up Memory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8ZnLHPnArCz"
      },
      "source": [
        "You can use a memory buffer to store conversation history and include it in future prompts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qC7tJ7f3ArCz"
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain import ConversationChain\n",
        "\n",
        "# Initialize summarization-based memory\n",
        "memory = ConversationBufferMemory()\n",
        "\n",
        "# Clear memory\n",
        "memory.clear()\n",
        "\n",
        "# Create a chain with LLM and buffer memory\n",
        "chain = ConversationChain(llm=llm, memory=memory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtRn6qcNArC0"
      },
      "source": [
        "In this case, the second query (“What color was the dragon?”) will refer back to the dragon mentioned in the first query."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WO5XEDMeEQvc"
      },
      "source": [
        "### Step 2: Interact with the LLM with Summarization Memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDzr4SIsEH91"
      },
      "outputs": [],
      "source": [
        "# Start a conversation\n",
        "res1 = chain.run(\"Answer with short answer, give me a number between (0 and 9)\")\n",
        "res2 = chain.run(\"Answer with short answer, give me another number between (10 and 19)\")\n",
        "res3 = chain.run(\"From the previous conversations. What was these numbers and what is the result of adding them ?\")\n",
        "\n",
        "print(f'First Response: {res1}\\nSecond Response: {res2}\\nThird Response: {res3}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Xxb3a2JArC0"
      },
      "source": [
        "## LLM as Memory in LangChain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Wy_u90qArC0"
      },
      "source": [
        "LLM as memory allows us to query the language model to summarize or remember certain pieces of information from previous conversations. This is particularly useful when you're working with large datasets or conversations where storing all the information might not be practical."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAqSf-MeArC0"
      },
      "source": [
        "### Step 1: Set Up a Summary-based Memory System"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alZS3RdEArC0"
      },
      "source": [
        "LangChain includes a class that allows the use of summarization memory, where the LLM generates a summary of past interactions.\n",
        "\n",
        "Let's begin by setting up the ConversationSummaryMemory that uses the LLM to remember interactions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fNVNXaJWArC0"
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ConversationSummaryMemory\n",
        "from langchain import ConversationChain\n",
        "\n",
        "# Initialize summarization-based memory\n",
        "summary_memory = ConversationSummaryMemory(llm=llm)\n",
        "\n",
        "# Clear memory\n",
        "summary_memory.clear()\n",
        "\n",
        "# Create a chain with LLM and buffer memory\n",
        "chain_with_buffer_memory = ConversationChain(llm=llm, memory=summary_memory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMESm323ArC0"
      },
      "source": [
        "In this case, we use the `ConversationSummaryMemory` module, which will generate summaries of the conversations so that the model can use that summary for context in future interactions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52cEJVuRArC0"
      },
      "source": [
        "### Step 2: Interact with the LLM with Summarization Memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fac-yp8DArC0"
      },
      "outputs": [],
      "source": [
        "# Start a conversation\n",
        "res1 = chain_with_buffer_memory.run(\"Answer with short answer, give me a number between (0 and 9)\")\n",
        "res2 = chain_with_buffer_memory.run(\"Answer with short answer, give me another number between (10 and 19)\")\n",
        "res3 = chain_with_buffer_memory.run(\"From the previous conversations. What was these numbers and what is the result of adding them ?\")\n",
        "\n",
        "print(f'First Response: {res1}\\nSecond Response: {res2}\\nThird Response: {res3}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyLIiLkTArC0"
      },
      "source": [
        "# Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdrsAjb-ArC0"
      },
      "source": [
        "## By incorporating LLM as memory, LangChain allows for:\n",
        "\n",
        "* **Enhanced recall** of previous conversations, enabling long-term context in conversations.\n",
        "* **Dynamic summarization** that uses the LLM itself to remember key points.\n",
        "* **Efficient memory management**, especially useful for complex, multi-turn interactions where remembering everything is unnecessary.\n",
        "\n",
        "## Recap of What We’ve Learned:\n",
        "* **Basic LLM Queries:** Interacting with GPT models using LangChain.\n",
        "* **Agents:** Creating intelligent agents that make decisions on which actions to take.\n",
        "* **Memory Buffers:** Storing conversation history using buffers.\n",
        "* **LLM as Memory:** Summarizing and recalling past interactions using the language model itself.\n",
        "* **Combining Tools and Memory:** Creating agents that use both memory and external tools for dynamic, real-world tasks.\n",
        "\n",
        "With LangChain’s memory features, you can build more intelligent and context-aware applications that retain conversation history, summarize large datasets, and combine external data sources effectively."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
